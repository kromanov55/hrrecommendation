{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3e5a399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.87        99\n",
      "           1       0.84      0.89      0.86        87\n",
      "\n",
      "    accuracy                           0.87       186\n",
      "   macro avg       0.87      0.87      0.87       186\n",
      "weighted avg       0.87      0.87      0.87       186\n",
      "\n",
      "ROC AUC Score: 0.9464762568210844\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Проверка доступности GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Читаем json файл с резюме и вакансиями\n",
    "with open('data.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#Извлекаем отклонённые резюме\n",
    "df_f = pd.json_normalize(data, record_path=['failed_resumes'], meta=[['vacancy', 'uuid'], ['vacancy', 'name'], ['vacancy', 'keywords'], ['vacancy', 'description'], ['vacancy', 'comment']])\n",
    "#Извлекаем принятые резюме\n",
    "df_c = pd.json_normalize(data, record_path=['confirmed_resumes'], meta=[['vacancy', 'uuid'], ['vacancy', 'name'], ['vacancy', 'keywords'], ['vacancy', 'description'], ['vacancy', 'comment']])\n",
    "#Добавляем коды статусов\n",
    "df_f['status'] = 0\n",
    "df_c['status'] = 1\n",
    "#Собираем в единый датасет\n",
    "frames = [df_f, df_c]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "# Преобразование дат рождения в возраст\n",
    "df['birth_date'] = pd.to_datetime(df['birth_date'], errors='coerce')\n",
    "df['age'] = (pd.Timestamp('now') - df['birth_date']).dt.days // 365\n",
    "\n",
    "# Заполнение пропусков\n",
    "df['city'].fillna('Unknown', inplace=True)\n",
    "df['age'].fillna(df['age'].mean(), inplace=True)\n",
    "\n",
    "# Кодирование категориальных данных\n",
    "label_encoders = {}\n",
    "categorical_columns = ['country', 'city', 'vacancy.uuid', 'vacancy.name']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Загрузка токенизатора и модели BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased').to(device)\n",
    "\n",
    "def get_bert_embeddings_batch(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Применение BERT для текстовых столбцов партиями\n",
    "text_columns = ['about', 'key_skills', 'vacancy.description']\n",
    "batch_size = 32\n",
    "for col in text_columns:\n",
    "    embeddings = []\n",
    "    texts = df[col].astype(str).fillna('').tolist()\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_embeddings = get_bert_embeddings_batch(batch_texts)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    df_bert = pd.DataFrame(embeddings, columns=[f\"{col}_bert_{i}\" for i in range(embeddings.shape[1])])\n",
    "    \n",
    "    # Сброс индексов перед конкатенацией\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df_bert.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df = pd.concat([df, df_bert], axis=1)\n",
    "\n",
    "# Удаление ненужных столбцов\n",
    "drop_columns = ['uuid', 'first_name', 'last_name', 'birth_date', 'about', 'key_skills', 'experienceItem', 'educationItem', 'languageItems', 'vacancy.description', 'vacancy.keywords', 'vacancy.comment', 'languageItem']\n",
    "df.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "# Балансировка классов\n",
    "df_majority = df[df.status == 0]\n",
    "df_minority = df[df.status == 1]\n",
    "\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # Дублирование меньшинства\n",
    "                                 n_samples=len(df_majority),    # Доведение до размера большинства\n",
    "                                 random_state=123) # Случайный сид\n",
    "\n",
    "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Разделение данных на признаки и целевую переменную\n",
    "X = df_balanced.drop(columns=['status'])\n",
    "y = df_balanced['status']\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Масштабирование признаков\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Оптимизация гиперпараметров с помощью GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=GradientBoostingClassifier(random_state=42), param_grid=param_grid, cv=5, scoring='roc_auc')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Предсказание и оценка модели\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_pred_proba)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba103737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'best_model.pkl')\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee75363d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.68      0.59        99\n",
      "           1       0.43      0.28      0.34        87\n",
      "\n",
      "    accuracy                           0.49       186\n",
      "   macro avg       0.47      0.48      0.46       186\n",
      "weighted avg       0.47      0.49      0.47       186\n",
      "\n",
      "ROC AUC Score: 0.4427609427609428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('best_model.pkl')\n",
    "\n",
    "# Load the scaler\n",
    "loaded_scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# Now you can use loaded_model and loaded_scaler for predictions\n",
    "X_test_transformed = loaded_scaler.transform(X_test)\n",
    "y_pred = loaded_model.predict(X_test_transformed)\n",
    "y_pred_proba = loaded_model.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_pred_proba)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e3ba838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.87        99\n",
      "           1       0.84      0.89      0.86        87\n",
      "\n",
      "    accuracy                           0.87       186\n",
      "   macro avg       0.87      0.87      0.87       186\n",
      "weighted avg       0.87      0.87      0.87       186\n",
      "\n",
      "ROC AUC Score: 0.9464762568210844\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_pred_proba)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adf0f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open('best_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(best_model, model_file)\n",
    "\n",
    "# Save the scaler\n",
    "with open('scaler.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bd5787a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.68      0.59        99\n",
      "           1       0.43      0.28      0.34        87\n",
      "\n",
      "    accuracy                           0.49       186\n",
      "   macro avg       0.47      0.48      0.46       186\n",
      "weighted avg       0.47      0.49      0.47       186\n",
      "\n",
      "ROC AUC Score: 0.4427609427609428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the model\n",
    "with open('best_model.pkl', 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)\n",
    "\n",
    "# Load the scaler\n",
    "with open('scaler.pkl', 'rb') as scaler_file:\n",
    "    loaded_scaler = pickle.load(scaler_file)\n",
    "\n",
    "# Now you can use loaded_model and loaded_scaler for predictions\n",
    "X_test_transformed = loaded_scaler.transform(X_test)\n",
    "y_pred = loaded_model.predict(X_test_transformed)\n",
    "y_pred_proba = loaded_model.predict_proba(X_test_transformed)[:, 1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_pred_proba)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae751e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
